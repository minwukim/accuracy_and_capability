Anonymous github for EMNLP Submission.


# ğŸ§  RL vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning

This repository contains the code used in the paper.

## ğŸ“ Code Structure

```bash
.
â”œâ”€â”€ environment.yml           # ğŸ§ª Conda environment file
â”œâ”€â”€ zero3.yaml               # âš™ï¸ DeepSpeed Zero3 config for RLVR training
â”‚
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ RLVR/
â”‚   â”‚   â”œâ”€â”€ grpo_trainer.py     # RL training script using TRL (GRPO)
â”‚   â”‚   â””â”€â”€ grpoconfig.yaml     # Config file for GRPO training
â”‚   â”‚
â”‚   â””â”€â”€ Distillation/
â”‚       â”œâ”€â”€ sft_trainer.py      # Supervised fine-tuning script
â”‚       â””â”€â”€ sftconfig.yaml      # SFT config file
â”‚
â””â”€â”€ test/
    â””â”€â”€ MATH500_eval.py        # ğŸ¯ Evaluation script on MATH500
````

---

## âš™ï¸ Environment Setup

Create the conda environment and install dependencies:

```bash
conda env create -f environment.yml
conda activate rlvsdistill
```

Ensure your system supports [DeepSpeed](https://www.deepspeed.ai/) and has `accelerate` configured.

---

## ğŸš€ How to Run

### ğŸ” Evaluation (MATH500)

```bash
python test/MATH500_eval.py
```

---

### ğŸ§ª RLVR Training (GRPO via TRL + VLLM)

We use **TRL** for policy optimization and **VLLM** for fast multi-process inference during reward evaluation.

```bash
accelerate launch \
  --config_file zero3.yaml \
  --num_processes <NUM_PROCESSES> \
  train/RLVR/grpo_trainer.py
```

ğŸ’¡ **Note:**

* `<NUM_PROCESSES>` should be set to the number of available CPU cores **minus one (â€“1)**.
  This is because **VLLM** internally uses one process for fast batched model inference, and the remaining processes will be used for parallel reward evaluation.
* Example: If your machine has 16 logical cores, use `--num_processes 15`.

ğŸ› ï¸ Edit `train/RLVR/grpoconfig.yaml` to configure:

* Base model and reward model paths
* Dataset locations
* Sampling parameters
* Reward functions

---

### ğŸ“˜ Distillation Training (SFT)

```bash
python train/Distillation/sft_trainer.py --config train/Distillation/sftconfig.yaml
```

Use the config file to specify:

* Teacher and student model
* Number of training steps
* Prompt format and dataset
* Saving & logging behavior

---

## ğŸ“¦ Libraries Used

* ğŸ¤— **[Transformers](https://github.com/huggingface/transformers)** â€“ model loading and generation
* ğŸ¤— **[TRL](https://github.com/huggingface/trl)** â€“ for RL fine-tuning via GRPO
* âš¡ **[VLLM](https://github.com/vllm-project/vllm)** â€“ for fast batched generation and reward model inference
* ğŸ§ª **DeepSpeed** â€“ memory-efficient distributed training


